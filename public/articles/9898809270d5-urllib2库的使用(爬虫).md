# urllib2库的使用(爬虫)

> urllib2 `在python3中改为urllib.request`



## 基础代码

~~~python
import urllib.request
# 导入urllib2库

urllib2 = urllib.request

#通过urllib2.Request方法构造一个请求对象
request = urllib2.Request("http://www.baidu.com/")

# 向指定的url地址发送请求，并返回服务器响应的文件对象
response = urllib2.urlopen(request)


#read方法是读取文件中的全部内容，返回字符串
html = response.read()

print(html)
~~~





## 为了让服务器不知道自己是爬虫，需要设置请求头

~~~python
import urllib.request
# 导入urllib2库

urllib2 = urllib.request

#指定请求头某些属性,User-Agent也是反爬虫的其中一步
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"
}

#通过urllib2.Request方法构造一个请求对象
request = urllib2.Request("http://www.baidu.com/", headers=headers)

# 向指定的url地址发送请求，并返回服务器响应的文件对象
response = urllib2.urlopen(request)


#read方法是读取文件中的全部内容，返回字符串
html = response.read()


statusCode = response.getcode()
#返回响应码

url = response.geturl()
#返回实际数据的实际地址，防止重定向

info = response.info()
#返回响应报头信息
~~~





## 进一步加强反爬虫机制

~~~python
import urllib.request
# 导入urllib2库
import random
urllib2 = urllib.request

#指定请求头某些属性

url = "http://www.baidu.com/"


# 可以是user-agent列表，也可以是代理列表
ua_list = [
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36",
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11",
"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16",
"Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50",
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50"
]

# 在列表中随机选择一个User-Agent
user_agent = random.choice(ua_list)


# 构造一个请求对象
request = urllib2.Request(url)


# 给请求对象添加一个http报头
request.add_header("User-Agent", user_agent)

# 获取一个已有的报头的值，注意只能第一个字母大写
request.get_header("User-agent")


response = urllib2.urlopen(request)


#read方法是读取文件中的全部内容，返回字符串
html = response.read()

statusCode = response.getcode()
#返回响应码

url = response.geturl()
#返回实际数据的实际地址，防止重定向

info = response.info()
print(html,statusCode,url,info)

~~~





## urlencode将url上的汉字转为Unicode编码

~~~python
import urllib.request

urllib2 = urllib.request


wd = {"name": 'fyh'}
#以下为python2的方法
urllib2.urlencode(wd)
#name=%.....
#转为键值对形式

#以下为python2的方法
urllib.parse.urlencode(values)
~~~



## 以百度url地址为借接口，爬取百度到的页面

~~~python
import urllib.request
import urllib
url_lib = urllib.request

url = "http://www.baidu.com/s"

keyword = input("请输入需要查询的字符串")

headers = {"User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36"}

wd = {"wd": keyword}
wd = urllib.parse.urlencode(wd)


fullurl = url + "?" + wd

request = url_lib.Request(fullurl, headers=headers)

response = url_lib.urlopen(request)

html = response.read()
f = open('./demo.html', 'wb')
f.write(html)
f.close()
~~~







### 函数式编程，爬取贴吧数据

~~~python
import urllib.request
import urllib
from threading import Thread
def loadPage(url,filename):
    """
    :param url:需要爬取的url地址
    作用：根据url发送请求，获取服务器响应文件
    """
    print("正在下载" + filename)
    headers = {"User-Agent": "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50"}
    request = urllib.request.Request(url, headers=headers)
    response = urllib.request.urlopen(request)
    return response.read()


def writePage(html,filename):
    """
    :param html:服务器响应文件内容
    作用：将Html内容写入到本地
    """

    print("正在保存" + filename)
    with open(filename, 'wb') as f:
        f.write(html)

def tiebaSpider(url, beginPage, endPage):
    """
    作用：贴吧爬虫调度器
    """
    for page in range(beginPage, endPage + 1):
        pn = (page-1)*50
        filename = "./spiderFile/" + "第" + str(page) + "页.html"
        fullurl = url + "&pn=" + str(pn)
        print(fullurl)
        html = loadPage(fullurl, filename)
        writePage(html, filename)
        print("谢谢使用")

if __name__ == '__main__':
    kw = input("请输入需要爬取的贴吧名：")
    beginPage = int(input("请输入起始页："))
    endPage = int(input("输入结束页:"))
    url = "http://tieba.baidu.com/f"
    key = urllib.parse.urlencode({"kw":kw})
    fullurl = url + "?" + key
    tiebaSpider(fullurl, beginPage, endPage)
~~~



## 使用抓包工具获取post表单数据格式，发送post请求

~~~python
#!/usr/bin/env python
# -*- coding:utf-8 -*-

import urllib.request
import urllib

# 通过抓包的方式获取的url，并不是浏览器上显示的url
url = "http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule&smartresult=ugc&sessionFrom=null"

# 完整的headers
headers = {
        "Accept" : "application/json, text/javascript, */*; q=0.01",
        "X-Requested-With" : "XMLHttpRequest",
        "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",
        "Content-Type" : "application/x-www-form-urlencoded; charset=UTF-8",
    }

# 用户接口输入
key = input("请输入需要翻译的文字:")

# 发送到web服务器的表单数据,需要用抓包工具
formdata = {
"from" : "AUTO",
"to" : "AUTO",
"i" : key,
"smartresult" : "json",
"xmlVersion" : "1.8",
"keyfrom" : "fanyi.web",
"ue" : "UTF-8",
"action" : "FY_BY_CLICKBUTTON",
"typoResult" : "true"
}

# 经过urlencode转码
data = urllib.parse.urlencode(formdata)

# 如果Request()方法里的data参数有值，那么这个请求就是POST
# 如果没有，就是Get
request = urllib.request.Request(url, data = data, headers = headers)

print(urllib.request.urlopen(request).read())


~~~



## Ajax加载方式的数据获取

~~~python
#!/usr/bin/env python
# -*- coding:utf-8 -*-

import urllib
import urllib2

url = "https://movie.douban.com/j/chart/top_list?type=11&interval_id=100%3A90&action"

headers = {"User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}

formdata = {
        "start":"0",
        "limit":"20"
    }

data = urllib.urlencode(formdata)

request = urllib2.Request(url, data = data, headers = headers)

print urllib2.urlopen(request).read()
~~~





## 用抓包工具获取cookies模拟登录

~~~python
#!/usr/bin/env python
# -*- coding:utf-8 -*-

import urllib.request

url = "http://i.huya.com/"

headers = {
    "Host" : "http://i.huya.com/",
    "Connection" : "keep-alive",
    #"Upgrade-Insecure-Requests" : "1",
    "User-Agent" : "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36",
    "Accept" : "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Referer" : "http://www.huya.com/g/cf",
    #"Accept-Encoding" : "gzip, deflate, sdch",
    "Cookie" : "__yamid_tt1=0.10712440030102344; __yamid_new=C7F8EB3D35800001352421511AC02F30; first_username_flag=newqq_tkg91851c_first_1; isInLiveRoom=true; videoBitRate=8500; alphaValue=0.80; udb_guiddata=70bc299596b84757a548adf75b338f61; guid=7160c3b15d8d8a5b7df92e3f2f3fa3fd; vplayer_subscribe_banner_showed_83486985_2674462572=1; vplayer_subscribe_banner_showed_89008215_2648803790=1; vplayer_subscribe_banner_showed_27899470_2645015584=1; vplayer_subscribe_banner_showed_78941969_2656835446=1; vplayer_subscribe_banner_showed_1417676516_2691340440=1; vplayer_subscribe_banner_showed_67547117_2578948724=1; vplayer_subscribe_banner_showed_90883382_2573198696=1; vplayer_subscribe_banner_showed_99173389_99173389=1; vplayer_subscribe_banner_showed_117432206_2674313764=1; vplayer_subscribe_banner_showed_1420653764_1420653764=1; avatar=http://thirdqq.qlogo.cn/qqapp/101406359/7E924A6FD59F6C5BBC7801FACC486EB7/100; nickname=%F0%9F%8D%83%EF%BC%B3%D0%81v%E2%91%A6%C4%93%D0%99; partner_uid=7E924A6FD59F6C5BBC7801FACC486EB7; udb_biztoken=AQCdvtw9IQ1EpEuV0tRIB2pxXYKDqTCD3LbqpwQ2H_oOV8sKSnk8mNiLHgNTqfyDlkl0Vpe8edYuPZ0cN19KU5UdNKyCIpsTi5I57weaqfjBnFq5H35qdfRUnuMF4rdUd3yD3WHT_4W7yXCHQ1Y4BF_fotv6kMXY-ciW2v0Mr4wkzdyA_62Y98pF6jzLkU3TQKH3VwitthPfvPcGcW0zjf9YELEHTx7fbTN08knLfGHh96Qd4rwCoG0yxemddVFOGFgImZEW0w-W3dwqKi8NnjzIe1dEYgjpfTsqp5zy7-3iz5DwWrPG49ySy-n928PNSCcHJoCiixPv2m-K7ZdxK4YL; udb_openid=7E924A6FD59F6C5BBC7801FACC486EB7; udb_origin=101; udb_other=%7B%22lt%22%3A%221536452548408%22%2C%22isRem%22%3A%221%22%7D; udb_passport=newqq_tkg91851c; udb_status=0; udb_uid=1602802837; udb_version=1.0; username=newqq_tkg91851c; yyuid=1602802837; lType=qq; 
    
 ...以下省略
}

request = urllib.request.Request(url, headers = headers)

response = urllib.request.urlopen(request)
f = open("./虎牙页面.html", "wb")
f.write(response.read())
f.close()


~~~





## 忽略SSL认证

~~~
import urllib.request
import ssl
context = ssl._create_unverified_context()
request = urllib.request.Request("http://www.baidu.com/")
response = urllib.request.urlopen(request, context)
html = response.read()
~~~

